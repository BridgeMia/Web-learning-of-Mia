# 正则化

机器学习中的正则化（L1, L2）的概念，从线性代数开始

## 线性代数中的正则化

假设我们现在的数据都是向量，我们首先需要介绍向量的范数的概念。

### 向量的范数(norm)

向量范数的定义为：
$$
\parallel \vec x \parallel _p = (\sum_{i=1}^n{\mid x_i \mid ^p})^ \frac{1}{p}
$$
也就是向量中的每个元素的绝对值的$p$次方的和的 $\frac{1}{p}$ 次方，一般来说用到的比较多的是:

1. 0-范数： $p=0$

   是向量中非0元素的个数

2. 1-范数：$p=1$

   向量中所有元素的绝对值的和：
   $$
   \parallel \vec x \parallel _1 = (\sum_{i=1}^n{\mid x_i \mid})
   $$

3. 2-范数：$p=2$

   向量的模：
   $$
   \parallel \vec x \parallel _2 = \sqrt{(\sum_{i=1}^n{\mid x_i \mid ^2})}
   $$

4. 无穷范数：$p=\infty$

   向量中的元素绝对值的最大值

5. 负无穷范数：$p= - \infty$

   向量中的元素绝对值的最小值

代码部分：`np.linalg.norm`

```python
import numpy as np

array = np.array((range(1, 10))) - 5
print(array)
# Output:
# [-4 -3 -2 -1  0  1  2  3  4]

# 绝对值的最小值
print(np.linalg.norm(array,-np.inf))
# Out:
# 0.0

# 绝对值的最大值
print(np.linalg.norm(array,np.inf))
# Out:
# 4.0

# 非零元素个数
print(np.linalg.norm(array,0))
# Out:
# 8.0

# 绝对值的和

```



### 向量的规范化(normalize)

