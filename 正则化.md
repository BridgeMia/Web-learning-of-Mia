# 正则化

机器学习中的正则化（L1, L2）的概念，从线性代数开始

## 线性代数中的规范化

假设我们现在的数据都是向量，我们首先需要介绍向量的范数的概念。

### 向量的范数(norm)

向量范数的定义为：
$$
\parallel \vec x \parallel _p = (\sum_{i=1}^n{\mid x_i \mid ^p})^ \frac{1}{p}
$$
也就是向量中的每个元素的绝对值的$p$次方的和的 $\frac{1}{p}$ 次方，一般来说用到的比较多的是:

1. 0-范数： $p=0$

   是向量中非0元素的个数

2. 1-范数：$p=1$

   向量中所有元素的绝对值的和：
   $$
   \parallel \vec x \parallel _1 = (\sum_{i=1}^n{\mid x_i \mid})
   $$

3. 2-范数：$p=2$

   向量的模：
   $$
   \parallel \vec x \parallel _2 = \sqrt{(\sum_{i=1}^n{\mid x_i \mid ^2})}
   $$

4. 无穷范数：$p=\infty$

   向量中的元素绝对值的最大值

5. 负无穷范数：$p= - \infty$

   向量中的元素绝对值的最小值

代码部分：`np.linalg.norm`

```python
import numpy as np

array = np.array((range(1, 10))) - 5
print(array)
# Output:
# [-4 -3 -2 -1  0  1  2  3  4]

# 绝对值的最小值
print(np.linalg.norm(array,-np.inf))
# Out:
# 0.0

# 绝对值的最大值
print(np.linalg.norm(array,np.inf))
# Out:
# 4.0

# 非零元素个数
print(np.linalg.norm(array,0))
# Out:
# 8.0

# 绝对值的和
print(np.linalg.norm(array, 1))
# Out:
# 20.0

# 平方和的1/2次方
print(np.linalg.norm(array, 2))
# Out:
# 7.745966692414834


```



### 向量的规范化(normalize)

有了范数之后，向量就可以规范化，就得到了一个取值范围在(0, 1)的向量。
$$
l-p-normalized vector = [\frac{x_i}{\parallel \vec x \parallel _p}]
$$
在实际使用的时候一般用到的是l1和l2, 在keras中有专门的函数，其实也比较简单：

```python
from keras.utils import normalize
import numpy as np

array_sp = np.array([1, 2, 2, 0])

l1_normalized_array = normalize(array_sp, axis=-1, order=1)
l2_normalized_array = normalize(array_sp, axis=-1, order=2)

print(array_sp)
print(l1_normalized_array)
print(l2_normalized_array)

# Out:
# [1 2 2 0]
# [[0.2 0.4 0.4 0. ]]
# [[0.33333333 0.66666667 0.66666667 0.        ]]

```



## 机器学习中的正则化

### 数学意义

机器学习中的正则化的目的是限制模型的参数，防止过拟合，实际上是分别限制了模型参数构成的向量的范数：

L2正则化：


$$
L = E_{original} + \lambda \sum_j \omega_j^2 
$$
L1正则化：
$$
L = E_{original} + \lambda \sum_j \mid \omega_j \mid
$$
物理意义因为需要理解梯度下降，所以暂时不解释，可以参考这这篇文章：

[【通俗易懂】机器学习中 L1 和 L2 正则化的直观解释](https://blog.csdn.net/red_stone1/article/details/80755144)



### keras实现

keras中有专门的正则化器(regularizer)来实现正则化，**正则化器允许在优化过程中对层的参数或层的激活情况进行惩罚。 网络优化的损失函数也包括这些惩罚项**。

在一般的层中，有三种：

- `kernel_regularizer`：施加在权重上的正则项，为`keras.regularizer.Regularizer`对象
- `bias_regularizer`：施加在偏置向量上的正则项，为`keras.regularizer.Regularizer`对象
- `activity_regularizer`：施加在输出上的正则项，为`keras.regularizer.Regularizer`对象

例如：

```python
from keras import regularizers
model.add(Dense(64, input_dim=64,
                kernel_regularizer=regularizers.l2(0.01),
                activity_regularizer=regularizers.l1(0.01)))

```

这样就分别在权重和偏置项上加上了正则项。传入的参数是上一节提到的$\lambda$



