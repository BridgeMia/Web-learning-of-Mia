# Word to Vector

 ## 词向量

- 词向量是一种表示自然语言中的词语的形式，是自然语言处理中比较基础的一步预处理，如果把词向量理解为一个函数的话，那么他的输入是一个自然语言的词，是一个字符串，输出则是一个指定维度的向量。

- 常用的两种把词转换为词向量的方法是利用神经网络的embedding层，或者使用`gensim`包中的`Word2Vec`模块。

- 评价指标

  1. 对于一个独立的word2vec系统，评价的指标是语义相似的词语在编码后的词向量空间中的距离也近。

  2. 对于作为神经网络的一部分的word2vec，它只需要让编码后的向量输入到后面的层中能让模型有更好的表现就足够了，具体可以参见embedding层的解释。



## 神经网络的embedding层

神经网络的embedding层可以看做是将数字编码成一个指定维度的向量的层.

注意因为神经网络的输入都是数字，所以在利用embedding层之前还需要把词语编码，也就是一个词语到数字的映射，我们可以称之为word to ID, 然后就是对这些ID的编码，得到的是一个向量。

### embedding之后

embedding层实现的功能，跟它后面接的是什么有关系，如果需要一个通用的word2vec系统，那么后面就需要接一个用来评价word2vec好坏标准的神经网络，对应词向量评价标准的第一条；如果这个embedding层只是你的神经网络的一部分，那么他只需要从数字中提取出对后面的模型有用的信息就足够了，至于提取出来的是什么信息，一是跟后面的模型有关，二是比较难以评价。

### 怎么使用

我们以keras为例

```python
from keras.layers import Embedding, Dense
from keras.models import Sequential

words = ['苹果', '梨', '锅盖', '茄子', '菜刀']
word2id = {x[0]:x[1] for x in enumerate(words)}

vocab_size = len(words)+1
emb = Embedding(vocab_size, 50)

model = Sequential()
model.add(emb)

model.compile(optimizer='adam', loss='mse')

ret = model.predict([0, 1, 2, 3, 4])
print(ret)
# Output is a np.ndarray with shape: (5)
```

### 缺点

缺点就是现成的word2vec模型大多数不是用embedding层实现的。

## Gensim

gensim是最常用的word2vec模型，当然他的功能不仅仅是word2vec, 这里我们只介绍他的word2vec的使用。

### 使用

1. `gensim.models.word2vec.Word2Vec`和`gensim.models.Word2Vec`

   这两个容易搞混，他们都能实现word to vector的功能，但是后者可以看做一个简化版的模型，可以认为是只包含了word到vector的映射关系，而没有模型的其他信息，为了模型可以继续训练（后面会讲到），我们一般使用前者


2. 使用

   参数说明

   ```python
   gensim.models.word2vec.Word2Vec(sentences=None,
                                   size=100,
                                   alpha=0.025,
                                   window=5, 
                                   min_count=5, 
                                   max_vocab_size=None, 
                                   sample=0.001, 
                                   seed=1, 
                                   workers=3, 
                                   min_alpha=0.0001, 
                                   sg=0, hs=0, 
                                   negative=5, 
                                   cbow_mean=1, 
                                   hashfxn=<built-in function hash>,
                                   iter=5,
                                   null_word=0, 
                                   trim_rule=None, 
                                   sorted_vocab=1, 
                                   batch_words=10000)
   """
   主要参数：
   sentences: 用于训练的语料，格式为词语组成的句子的集合
   size: 映射出来的向量的长度
   alpha: 学习率
   window: 表示当前词与预测词在一个句子中的最大距离是多少，可以理解为相互有关系的词的最大距离
   sg: 训练算法： 0对应CBOW，1对应skip-gram算法，一般用1
   hs: hierarchica-softmax是否被启用，如果不启用（0）的话negative sampling会被使用
   iter: 遍历次数
   min_count: 词频少于这个数字的会被舍弃
   workers: 线程数，需要cython支持
   """
   
   ```

   一个例子：

   ```python
   from gensim.models.word2vec import Word2Vec
   import jieba
   
   # Test samples
   corpus1 = ['帮我关掉借呗可以吗', '蚂蚁借呗提前还了分期，到期后还要还这期吗']
   
   # 语料是一个词构成的句子的集合
   corpus1 = [[x for x in jieba.cut(y)] for y in corpus1]
   
   # 用corpus1创建一个模型
   
   mod1 =  Word2Vec(corpus1, size=64, window=10, min_count=1, iter=10)
   print(mod1['可以'])
   
   # Output:
   """
   [ 0.00160765  0.00088953 -0.00372339 -0.00672681 -0.00455872 -0.00428723
    -0.00150453  0.00162154 -0.00416607 -0.00337227 -0.00411569  0.00428961
    -0.00674325  0.00328936 -0.0047722   0.00658594  0.00727128  0.00545137
    -0.00310788  0.00150946  0.00757949 -0.0073382  -0.00670347 -0.00742038
     0.00435547  0.00166862  0.00033934 -0.00159502 -0.00192862  0.00212765
    -0.00256022  0.00116785 -0.00154004 -0.00422251  0.00764872 -0.0063951
     0.00628084  0.00172573  0.0002716  -0.00759348  0.00662902  0.00588745
    -0.00235865 -0.00537811 -0.00742104  0.0037892   0.00779658 -0.00747149
     0.0041181   0.00766934  0.00668247 -0.0038505   0.00571261 -0.0003444
     0.00537034  0.0056195   0.00613567 -0.00489639 -0.00374866  0.00431086
     0.0020911  -0.00185487 -0.00491951 -0.00591551]
   """
   
   # 保存模型
   mod1.save(r'data/w2vmodel/word2vec_mod1')
   
   ```

   读取模型并更新

   ```python
   corpus2 = ['天猫店都能用花呗吗', '我用花呗买东西，隔天就还款了。为什么现在又扣款']
   corpus2 = [[x for x in jieba.cut(y)] for y in corpus2]
   
   # 读取模型
   mod2 = Word2Vec.load(r'data/w2vmodel/word2vec_mod1')
   
   try:
       print(mod2['买'])
   except:
       print('not in vocabulary')
      
   # Output:
   # not in vocabulary
   
   # 用corpus2构建新的语料，并继续训练
   mod2.build_vocab(corpus2, update=True)
   mod2.train(corpus2, total_examples=mod2.corpus_count, epochs=mod2.iter)
   
   try:
       print(mod2['买'])
   except:
       print('not in vocabulary')
   # Output:
   """
   [ 7.0279534e-03  1.9423374e-03 -7.2553055e-03  3.1736563e-03
     7.3760855e-03 -7.7250218e-03  3.5588508e-03 -6.6526597e-03
     4.0133530e-04 -3.0276652e-03  3.0577960e-03 -7.3455097e-03
    -2.2950401e-03 -1.5116556e-03  1.8927215e-03 -2.5222688e-03
     5.6182994e-03  8.2737231e-04  1.2505874e-03  1.1636070e-03
     5.1062083e-04 -2.8767127e-03  4.8779871e-04 -1.4338739e-03
     1.0334431e-03 -1.4889379e-03 -1.9407466e-04  5.3017600e-03
     4.7373604e-03 -1.3160831e-03 -8.9937763e-04 -3.2462366e-03
    -6.2558809e-03  7.0846053e-03 -5.4714549e-03 -3.1102884e-03
    -2.9039585e-03  1.3909242e-03 -4.3966221e-03 -4.8968787e-03
     3.9340467e-03  4.6374751e-03 -6.1596539e-03  5.8664731e-03
    -7.0561348e-03  4.9033510e-03  6.2037031e-03 -7.6445607e-03
    -7.4777422e-03  6.3564256e-03  6.2948992e-03  8.0381212e-04
     3.2370922e-04 -3.6755991e-03 -2.5527645e-03  3.8834361e-03
     4.6349294e-03 -7.1200733e-03 -5.8448715e-03  2.6123102e-03
     4.7915843e-03 -6.1566971e-04 -9.8224962e-05  7.3243580e-03]
   """
   ```


### 缺点

word2vec的缺点是，生成的模型的主体是一个词-向量的映射关系，对于一个确定的模型，如果输入的词是训练的语料中没有的，就没办法找到相应的词向量。



## 两种方式的优缺点比较

1. embedding的优缺点：embedding的缺点除了上面所说的现成的模型少以外，还有个缺点是需要定义后续的模型来实现embedding层的训练得到合适的参数。优点是，这个模型保存的是模型的结构和参数，所以对于新加入的词，也可以得到词向量，虽然置信度并不是很高，但是好在可以直接用。
   - 解释：在这篇文档里面，我们word2id的方法是对word和id作一个一一映射，这个映射是随机的，所以id并没有其他的信息，仅仅是记录了word的id，新进来的词语对应的是新的数字，神经网络是对这个没有任何其他信息的数字作了映射，那么最后得到的结果也就并不是这个词的真正的意思。
   - 一个解决方法：另一种较为通用的word2id的方法是通过词频来代表一个词，这样有好处有坏处，好处是这个id不再是word的id, 而是这个词在语料中出现的次数，这样就增加了这个id包含的信息，这样这个数字就是有意义的数字了，相应的会损失一些信息：一些词频低的词可能词频是一样的，这样他们的区分就没有了。但是没太大的关系，因为频率太低的词并不是很重要，而词频高的词很大概率ID是独有的。事实上有些处理的过程中，词频很低的词都会被过滤掉，比如上面的gensim的word2vec模型中就有`min_count`参数。

2. gensim的优缺点：缺点比较明显，对于没出现的词没办法给出词向量，优点也比较明显，简单易用，模型的训练只需要准备语料就可以了。有很多现成的训练好的模型。